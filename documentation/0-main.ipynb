{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/uol-mediaprocessing-2023-2024-classroom/project-interaction-by-facial-expressions/blob/update-documentation/documentation/0-main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Documentation: Interaction by Facial Expression\n",
    "**Group members:** Lea Krawczyk​, Thi Mai Linh Nguyen​, Sebastian Vittinghoff​\n",
    "\n",
    "**Last update:** 14.02.2024\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "5sUXoTfNeXvy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Table of contents\n",
    "\n",
    ">[Documentation: Interaction by Facial Expression](#scrollTo=5sUXoTfNeXvy)\n",
    "\n",
    ">[Task distribution](#scrollTo=QR7GAceKwH5c)\n",
    "\n",
    ">[Progress Summary](#scrollTo=O8nNgMmstbWp)\n",
    "\n",
    ">>[08.11.2023](#scrollTo=O8nNgMmstbWp)\n",
    "\n",
    ">>[22.11.2023](#scrollTo=O8nNgMmstbWp)\n",
    "\n",
    ">>[06.12.2023](#scrollTo=O8nNgMmstbWp)\n",
    "\n",
    ">>[20.12.2023](#scrollTo=O8nNgMmstbWp)\n",
    "\n",
    ">>[16.01.2024](#scrollTo=O8nNgMmstbWp)\n",
    "\n",
    ">>[30.01.2024](#scrollTo=O8nNgMmstbWp)\n",
    "\n",
    ">[Known Issues & Limitations](#scrollTo=9ZT9FiiRwH5d)\n",
    "\n",
    ">[Outlook & Optimization thoughts](#scrollTo=jM-2BWQgwH5h)\n",
    "\n"
   ],
   "metadata": {
    "colab_type": "toc",
    "id": "4KDBi-7GK1Op"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task distribution <a class=\"anchor\" id=\"task-distribution\"></a>\n",
    "\n",
    "## Lea\n",
    "\n",
    "- Concept\n",
    "- Filters\n",
    "- Documentation\n",
    "- UI Design\n",
    "- Face & Eye Blinking Detection (Dlib & MediaPipe)\n",
    "- Gaze Detection\n",
    "\n",
    "## Mai Linh\n",
    "- Concept\n",
    "- Filters\n",
    "- Documentation\n",
    "- Face & Eye Blinking Detection (Dlib)\n",
    "- Emotion Detection (DeepFace) & speedtest\n",
    "\n",
    "## Sebastian\n",
    "- Concept\n",
    "- Filters\n",
    "- Documentation\n",
    "- Base Project & architecture\n",
    "- Dependency Management\n",
    "- Head pose detection (MediaPipe)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Progress Summary\n",
    "\n",
    "This section gives a brief summary of the progress of this project up to a specific date. Each progress date will be explained in detail in the files under this main-file in the documentation folder of this repository.\n",
    "\n",
    "## 08.11.2023\n",
    "\n",
    "The main focus was to organise our group and come to an agreement regarding the concept and requirements of our project.\n",
    "\n",
    "In order to keep track of our tasks and work distribution we decided to use an online project organization tool called [Trello](https://trello.com).\n",
    "We collectively decided to work on our own implementations of the frontend and backend. The structure of our repository includes a frontend, backend and documentation folder. We are going to use PyCharm as our main IDE.\n",
    "\n",
    "After the basic setup was completed we discussed our goals for this project and which requirements to implement (refer to [Main concept](#scrollTo=zeXdQ90Jix1i)).\n",
    "\n",
    "## 22.11.2023\n",
    "\n",
    "We created a basis for our project. In the backend we added the filters \"blur\", \"black and white\" and \"inverted\" and on the frontend we added a webcam, the help section, an action log and an area to modify the photos. Generally we experimented with filters of OpenCV and adjusted the parameters to our liking. As of now we can only apply the filters on example images in the modifying area. It's not possible to upload a photo yet. Furthermore there is no gallery implemented yet.\n",
    "\n",
    "## 06.12.2023\n",
    "\n",
    "We experimented with [dlib-Models](https://github.com/davisking/dlib-models) to choose an optimal model for eye and face detection. During our tests we added threshold values which are used to determine whether the left or right eye is closed. Additionally we tried to count the amount of \"blinking with the eyes\". This idea came up since we think that the amount of blinkings could be used for future commands.\n",
    "\n",
    "However, one main problem of this model was that the eye detection was not consistent so the threshold values were changing too much which in turn makes it harder to detect whether an eye is closed or not. In the future we want to research other models and adjust our implementation to get more consistent results of eye detection.\n",
    "\n",
    "## 20.12.2023\n",
    "\n",
    "Since the eye detection results of dlib are not consistent enough to our liking we looked into [Mediapipe](https://pypi.org/project/mediapipe/). This gave us similar results to Dlib at first until we discovered that settings can be changed within MediaPipe as well. Now that we experimented with this new model, we did get a more consistent detection of closed eyes. Additionally we looked into head movements and gaze tracking, also using MediaPipe, which we can use for additional commands.\n",
    "\n",
    "## 16.01.2024\n",
    "\n",
    "Our project focuses on operating a photo gallery and image manipulation website (almost) entirely by using the face and head. Therefore we looked into other interactions the face could have with our website. We settled on Emotion Detection and decided on using [deepface](https://pypi.org/project/deepface/) in our project to detect which emotion is displayed by the user. According to the user's emotion an emoji can be controlled. We decided against using emotion detection to navigate through our website since the detection is too inaccurate and would make controlling elements of our website frustrating.\n",
    "\n",
    "The UI design was also updated and layout improvements have been added. The Help section is now dynamic and has been updated with the available commands and its descriptions.\n",
    "\n",
    "## 30.01.2024\n",
    "\n",
    "During the implementation of the emotion detection functionality to our project we noticed a drop in processing speed of our website. So we conducted a speed test in which we test all available face detector backends of deepface (OpenCV, SSD, Dlib, MTCNN, Faster MTCNN, RetinaFace, MediaPipe, YOLOv8 Face, YuNet) with our website. The result is that RetinaFace and MTCNN is slowing down the application the most while OpenCv and a faster variant of MTCNN (facenet-pytorch) are the most consistent in detecting the correct emotion displayed by a user and need less than 1 second to detect a face and an emotion. Using RetinaFace or MTCNN in the background can take up to more than 20 seconds to display a change in emotion on the frontend which is not ideal for an application which relies on live-reactions to navigate through it.\n"
   ],
   "metadata": {
    "id": "O8nNgMmstbWp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Known Issues & Limitations\n",
    "\n",
    "- The Webcam starts lagging after a while.\n",
    "- If the camera is set too high or too low, it is recognized all the time that the head is either lowered or raised. This can lead to interactions being carried out that are not intended.\n",
    "- The emotion detection sometimes recognizes the wrong emotion."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Outlook & Optimization thoughts\n",
    "\n",
    "Our project could be improved in multiple ways.\n",
    "\n",
    "Concerning the emotion detection: Since deepface uses an AI model which was trained on the [FER2013](https://www.kaggle.com/datasets/msambare/fer2013) dataset we could improve the accuracy of the emotion recognition model by training our own model with our own datasets consisting of bigger images.\n",
    "\n",
    "Concerning performance speed: We need to optimize the communication between frontend and backend so the website can work accurately in real time. One possibility would be to analyze and process the images directly in the frontend to save the detour via a node such as the server. This would lead to a considerable performance boost. The resources that would have to be transferred for this would be OpenCV and MediaPipe. Another option would be to provide a remote server with increased computing power. This would be able to process the requests much more efficiently and take over the very computationally intensive analyses and process them faster.\n",
    "\n",
    "A useful extension for the project would be, for example, adding a setup step before starting the actual application. This would allow the user to assign the triggers to the interactions individually.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
