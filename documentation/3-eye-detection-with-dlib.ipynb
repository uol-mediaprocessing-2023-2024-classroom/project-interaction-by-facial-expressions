{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Documentation of the project Interaction by Facial Expressions\n",
    "**Group members:** Lea Krawczyk, Thi Mai Linh Nguyen, Sebastian Vittinghoff"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f74b37a5c395871a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Eye Detection with Dlib (06.12.2023)\n",
    "\n",
    "## Changing the goal\n",
    "Before our goal was to use facial expressions as well as gestures to navigate our application. After being given some feedback we decided to focus only on the facial expressions and make our application hands free. This helped us to narrow down our focus. \n",
    "We began researching what can be achieved by only using the face and wether there are any standards for using facial expressions for navigation.\n",
    "\n",
    "## Research results\n",
    "We started a generally researching standards for hands free applications. We did not find a universal gesture/facial expression guideline for navigation, however we found a few examples of hands free applications.  \n",
    "The first idea we came across, because he is a professor at our university was [hands free coding via voice control by Wolfram Wingerath](https://vsis-www.informatik.uni-hamburg.de/getDoc.php/publications/642/2021-ix-wingerath-handsfree-coding.pdf). This article states, that even though there is voice control or rather a dictation function on most of our devices it is often disregarded as a special \"fun\" feature, even though it is essential to a group of people.\n",
    "\n",
    "However since we are focusing on image processing by facial expressions and gestures, the idea wasn't really applicable to our project. We came across another hands free coding approach by Charlie Gerard. She developed an [extension for JetBrains IDE's which makes use of gaze detection](https://www.youtube.com/watch?v=0ISXpNJ5iNs). Even though the use case differs quite a lot from our use case we added gaze detection to our stack of navigation commands, since it seemed to be tried and tested and worked really well in her demonstration.  \n",
    "\n",
    "Another approach we came across revolves around eye tracking. Dr. Oualid S. wrote an [article on LinkedIn](https://www.linkedin.com/pulse/navigate-your-screen-blink-eye-oualid-soula) showing how he navigates the mouse using eye tracking. The special thing about this technique is, that the eye tracking is done without the use of external devices. He himself states, that it's not really the most precise and responsive way to navigate the computer. Since we were looking for high accuracy in our project we did not use anything from his approach but it was still interesting to see that this is actually possible without external devices. \n",
    "\n",
    "## Tests with Dlib models \n",
    "We started out by getting familiar with Dlib and learning how to utilize the landmark points to detect certain actions. For that we used the [shape_predictor_68_face_landmarks](https://github.com/davisking/dlib-models/blob/master/shape_predictor_68_face_landmarks.dat.bz2) by davisking to help us detect the eyes of a face. For the face detection itself,  we used the haarcascade_frontalface_default model from OpenCV. The first thing we built was a blink counter, using a [tutorial provided by Adrian Rosebrock](https://pyimagesearch.com/2017/04/24/eye-blink-detection-opencv-python-dlib/) and this article [Eye Aspect Ratio(EAR) and Drowsiness detector using Dlib](https://medium.com/analytics-vidhya/eye-aspect-ratio-ear-and-drowsiness-detector-using-dlib-a0b2c292d706) by Dhruv Pandey. \n",
    "Since we basically just copied and tried to understand the code from the tutorial we are not going to provide it here. The output we achieved was this:\n",
    "<br/>\n",
    "<div>\n",
    "    <img src=\"./assets/blink-counter.gif\" alt=\"blink counter output\" width=\"640\">\n",
    "</div>\n",
    "\n",
    "## Eye Detection\n",
    "Dlib detects 68 landmarks in a face with 37-42 being the landmarks of the right eye and 43-48 of the left eye. This means that each eye is represented by 6 landmark points. Those 6 points are put into a formula which calculates the Eye-Aspect-Ratio (EAR). \n",
    "The EAR works by using the euclidian distance between the upper and lower landmarks points of the eye. So we calculate (for the right eye): \n",
    "<p>$\\[ \\text{EAR} = \\frac{\\|38 - 42\\| + \\|39 - 41\\|}{2\\|37 - 40\\|} \\]$</p>\n",
    "\n",
    "The euclidian distance of the vertical points of the eye added together, divided by twice the euclidian distance of the horizontal points of the eye. How this formula works is also documented in these images:\n",
    "<br/>\n",
    "<div>\n",
    "    <img src=\"./assets/eye-aspect-ratio.png\" alt=\"EAR visually explained\" width=\"640\">\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2d7979ae4e71a16"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the image it is illustrated, that the colored vertical lines approach zero. The value of the EAR is higher when the eye is open and lower when it's closed. So the idea is to use a value as a threshold which we then check in an if-condition to determine whether an eye is closed or not."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dabdc0d6a8f52c1e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install dlib\n",
    "!pip install imutils\n",
    "!pip install scipy\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "\n",
    "EYE_AR_THRESH = 0.18\n",
    "EYE_AR_CONSEC_FRAMES = 15\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"./models/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "\n",
    "vs = cv2.VideoCapture(1)\n",
    "\n",
    "while True:\n",
    "    ret, frame = vs.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    rects = detector(gray, 0)\n",
    "\n",
    "    for rect in rects:\n",
    "\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "        for (x, y) in shape:\n",
    "            cv2.circle(frame, (x, y), 3, (0, 0, 255), -1)\n",
    "\n",
    "        leftEye = shape[lStart:lEnd]\n",
    "        rightEye = shape[rStart:rEnd]\n",
    "        leftEAR = eye_aspect_ratio(leftEye)\n",
    "        rightEAR = eye_aspect_ratio(rightEye)\n",
    "\n",
    "        ear = (leftEAR + rightEAR)\n",
    "\n",
    "        if (rightEAR < EYE_AR_THRESH) & (leftEAR > rightEAR):\n",
    "            cv2.putText(frame, \"Left eye closed\", (10, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        elif (leftEAR < EYE_AR_THRESH) & (rightEAR > leftEAR):\n",
    "            cv2.putText(frame, \"Right eye closed\", (200, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        elif ear < EYE_AR_THRESH:\n",
    "            cv2.putText(frame, \"Eye: {}\".format(\"both closed\"), (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "vs.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fd237e0308b0a3b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The results we got from this were not particularly stable. We had a lot of wrong results when trying to detect which eye is closed. We tried different thresholds:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b7c69fdccd39a69"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install dlib\n",
    "!pip install imutils\n",
    "!pip install scipy\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "\n",
    "EYE_AR_THRESH = 0.3\n",
    "EYE_AR_CONSEC_FRAMES = 15\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"./models/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "\n",
    "vs = cv2.VideoCapture(1)\n",
    "\n",
    "while True:\n",
    "    ret, frame = vs.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    rects = detector(gray, 0)\n",
    "\n",
    "    for rect in rects:\n",
    "\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "        for (x, y) in shape:\n",
    "            cv2.circle(frame, (x, y), 3, (0, 0, 255), -1)\n",
    "\n",
    "        leftEye = shape[lStart:lEnd]\n",
    "        rightEye = shape[rStart:rEnd]\n",
    "        leftEAR = eye_aspect_ratio(leftEye)\n",
    "        rightEAR = eye_aspect_ratio(rightEye)\n",
    "\n",
    "        ear = (leftEAR + rightEAR)\n",
    "\n",
    "        if (rightEAR < EYE_AR_THRESH) & (leftEAR > rightEAR):\n",
    "            cv2.putText(frame, \"Left eye closed\", (10, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        elif (leftEAR < EYE_AR_THRESH) & (rightEAR > leftEAR):\n",
    "            cv2.putText(frame, \"Right eye closed\", (200, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        elif ear < EYE_AR_THRESH:\n",
    "            cv2.putText(frame, \"Eye: {}\".format(\"both closed\"), (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "vs.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3369c1f335be0a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also tried different thresholds for the two eyes:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2bf9c0b810528d28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install dlib\n",
    "!pip install imutils\n",
    "!pip install scipy\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "from scipy.spatial import distance as dist\n",
    "\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "\n",
    "EYE_AR_THRESH = 0.35\n",
    "EYE_AR_CONSEC_FRAMES = 15\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"./models/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "\n",
    "vs = cv2.VideoCapture(1)\n",
    "\n",
    "while True:\n",
    "    ret, frame = vs.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    rects = detector(gray, 0)\n",
    "\n",
    "    for rect in rects:\n",
    "\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "        for (x, y) in shape:\n",
    "            cv2.circle(frame, (x, y), 3, (0, 0, 255), -1)\n",
    "\n",
    "        leftEye = shape[lStart:lEnd]\n",
    "        rightEye = shape[rStart:rEnd]\n",
    "        leftEAR = eye_aspect_ratio(leftEye)\n",
    "        rightEAR = eye_aspect_ratio(rightEye)\n",
    "\n",
    "        ear = (leftEAR + rightEAR)\n",
    "\n",
    "        if (rightEAR < 0.23) & (leftEAR > rightEAR):\n",
    "            cv2.putText(frame, \"Left eye closed\", (10, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        elif (leftEAR < 0.27) & (rightEAR > leftEAR):\n",
    "            cv2.putText(frame, \"Right eye closed\", (200, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        elif ear < EYE_AR_THRESH:\n",
    "            cv2.putText(frame, \"Eye: {}\".format(\"both closed\"), (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "vs.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2231277ead927e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The different thresholds seemed to have worked better but still not accurate enough. The accuracy was very dependent on environmental light and distance to the webcam. In the demo one can also see that the landmark points stick to the eyelid and do not move, even when closing an eye. Or in the other case the eye is not detected in its accurate position at all anymore. This can also be seen in the following image: \n",
    "<br/>\n",
    "<div>\n",
    "    <img src=\"./assets/eye-detection-dlib.png\" alt=\"eye detection inaccuracies using dlib\" width=\"640\"/>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9230f1613b8746ca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
