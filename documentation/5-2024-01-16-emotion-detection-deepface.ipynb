{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/uol-mediaprocessing-2023-2024-classroom/project-interaction-by-facial-expressions/blob/update-documentation/documentation/5-2024-01-16-emotion-detection-deepface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf2ad29e47052c84"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Emotion Detection with Deepface (16.01.2024)\n",
    "\n",
    "Our project uses [deepface](https://pypi.org/project/deepface/) to detect changes in facial expressions and then determine which emotion is the most dominant in an user's face."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cd4157f31f02b9e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deepface\n",
    "\n",
    "Deepface is a framework that can be used for face recognition and facial attribute analysis (age, gender, emotion, race). For our project we only use the function of recognising emotions.\n",
    "Before an emotion can be determined a face needs to be detected first. Deepface can use different face detectors in the backend: OpenCV, SSD, Dlib, MTCNN, Faster MTCNN, RetinaFace, MediaPipe, YOLOv8 Face and YuNet.\n",
    "While RetinaFace and MTCNN are more accurate in detecting and aligning faces they are rather slow. SSD and OpenCV are faster in detecting face.\n",
    "\n",
    "## Emotion Model\n",
    "As described in this [blog](https://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/) by Serengil, the model was trained on the [FER2013](https://www.kaggle.com/datasets/msambare/fer2013) dataset which consists of 48 x 48 pixel grayscale images. There are around 36000 images included in the dataset with seven emotion categories: angry, disgust, fear, happy, sad, surprise and neutral."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f593a9f4703ac12d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Application on images\n",
    "\n",
    "First deepface needs to be installed with this command:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "606938800cb09358"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pip install deepface"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3170d144de457f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code will analyse an image of Lenna, Angela Merkel and Albert Einstein and return the most dominant emotion detected."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99357ba051e67b9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "\n",
    "!wget -q https://raw.githubusercontent.com/uol-mediaprocessing/notebooks/master/pictures/Lenna.png\n",
    "!wget -q https://upload.wikimedia.org/wikipedia/commons/f/fd/Angela_Merkel_%282008%29.jpg\n",
    "!wget -q https://upload.wikimedia.org/wikipedia/commons/3/36/Bundesarchiv_Bild_183-19000-1918%2C_Albert_Einstein.jpg\n",
    "\n",
    "image_paths = [\"Lenna.png\", \"Angela_Merkel_(2008).jpg\", \"Bundesarchiv_Bild_183-19000-1918,_Albert_Einstein.jpg\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, path in enumerate(image_paths):\n",
    "    image = cv2.imread(path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    #Backends: opencv, ssd, dlib, mtcnn, retinaface, mediapipe, yolov8, yunet, fastmtcnn\n",
    "    \n",
    "    objs = DeepFace.analyze(image_rgb, actions=['emotion'], detector_backend='opencv')\n",
    "    emotion = objs[0]['dominant_emotion'][:]\n",
    "\n",
    "    face_coordinates = objs[0]['region']\n",
    "    x, y, w, h = face_coordinates['x'], face_coordinates['y'], face_coordinates['w'], face_coordinates['h']\n",
    "    cv2.rectangle(image_rgb, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    axes[i].imshow(image_rgb)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f\"Dominant Emotion: {emotion}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b965590221708e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "After detecting the face and extracting the facial features deepface uses a deep learning model to predict the emotion. If you execute the following code you can view the scores for each emotion of the first image in a diagram:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "979a3a5dd8de7310"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "image = cv2.imread(\"Lenna.png\")\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  \n",
    "\n",
    "emotions = DeepFace.analyze(image_rgb, actions=['emotion'], detector_backend='opencv')\n",
    "emotion_values = list(emotions[0]['emotion'].values())\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax1.imshow(image_rgb)\n",
    "ax1.axis('off')\n",
    "objects = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "y_pos = np.arange(len(objects))\n",
    "\n",
    "ax2.bar(y_pos, emotion_values, align='center', alpha=0.5)\n",
    "ax2.set_xticks(y_pos)\n",
    "ax2.set_xticklabels(objects)\n",
    "ax2.set_ylabel('Percentage')\n",
    "ax2.set_title('Emotion')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc6b095552562623"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since neutral has the highest score with around 70 % the face is classified as a neutral face."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad89b61b90a4327d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use in our project\n",
    "\n",
    "Instead of using emotions to trigger commands in our application we implemented emotion detection as a fun feature. The user can interact with a small ice bear emoji in the top right corner of the webcam while using the application. The ice bear will change its expression to the dominant emotion recognized on the user's face. The available emotions are: angry, fear, neutral, sad, disgust, happy and surprise.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/icebear_emoji_overview.jpg\" alt=\"Overview of emojis with photos\" width=\"500\"/>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdc7f50e4f9e18f6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
