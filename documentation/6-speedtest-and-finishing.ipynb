{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Documentation of the project Interaction by Facial Expressions\n",
    "**Group members:** Lea Krawczyk, Thi Mai Linh Nguyen, Sebastian Vittinghoff"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d856c949007fa22"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Speedtest & Finishing (30.01.2024)\n",
    "After implementing DeepFace and testing emotion detection with different backends we noticed that using some face detectors made the webcam footage lag and slow down other processes in our application. Therefore we conducted a Speedtest to determine which face detectors to avoid and which one to use for our application.\n",
    "\n",
    "## Speedtest\n",
    "The Speedtest was conducted on an Acer Spin 3 convertible with 16 GB RAM.\n",
    "We measured the time before and after executing this method in the image_analyzer in the backend:\n",
    "\n",
    "```python\n",
    " face_emotion_debouncer(lambda: handle_face_emotion(socketio, detect_face_emotion(image, face_detector_backend)))\n",
    "```\n",
    "\n",
    "This method processes the frames coming from the webcam in the frontend. It then returns the detected emotion to the frontend. This method needs to be wrapped in a Debouncer so it's not flooded with 60 frames per second from the webcam and tries to detect an emotion for every incoming frame. We recorded 10 times per face detector and calculated the average of those 10 times. The result of the speed test is in the table down below:\n",
    "<br/>\n",
    "<div\">\n",
    "    <img src=\"./assets/speedtest-results.jpg\" alt=\"Speedtest results\" width=\"640\"/>\n",
    "</div>\n",
    "\n",
    "RetinaFace and MTCNN in the backend where the slowest with more than 20 seconds for one result. Meanwhile Faster MTCNN was the fastest. For our project we did not want to have too many dependencies so we tried to avoid using Dlib, Faster MTCNN and YOLO since those face detectors needed to be installed separately. It's important to mention that the performance speed can vary depending on how long the application has been running. After starting the application the speed was the fastest but after around 1 to 2 minutes later the times will increase.\n",
    "\n",
    "### Emotion detection accuracy\n",
    "According to this [blog](https://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/) by Serengil who trained the emotion detection model for DeepFace. The model can be inaccurate at times and predict an emotion incorrectly. The confusion matrix down below is the result of his testings.\n",
    "<br/>\n",
    "<div>\n",
    "    <img src=\"./assets/confusion-matrix.jpg\" alt=\"Confusion matrix\" width=\"640\"/>\n",
    "</div>\n",
    "\n",
    "This means for example 467 angry faces were tested and out of them 214 were correctly labeled as an angry face. However, 9 angry faces were falsely predicted as a disgusted face. The results of this matrix aligns with our observations when we tested DeepFace in our application since the emotion detection can be inaccurate at times."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "642ee666accf0d73"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Additional features that has been added\n",
    "\n",
    "### Focus Mode\n",
    "The focus mode is described in a previous chapter but trying it in practice we decided to adjust a few things: \n",
    "When the application is started the user is locked in the first section of the application by default. If an area is locked on there will be a blue border around the area. If the area is unlocked the border is grey instead. To unlock a section the head needs to be either tilted up or both eyes need to be closed for two seconds. Our initial idea to close the eyes twice did not work within our code because we did not see a possibilty to properly count the blinks and only then trigger an action. The navigation through the application remains the same (tilting head left and right). \n",
    "When locked in on the carousel area the visible tab indication is not a border anymore but instead the filter buttons are being scaled up and the text is underlined. In this section we make use of the reusability of the head movements as well, because the user can navigate the filter menu the same way as they would the application. \n",
    "\n",
    "### Settings menu\n",
    "In the top right corner of the page there is now a settings menu. By clicking on it the whole menu will slide down. In this menu the user can activate a grid which marks the face and eye positions. There is also an option to show the emoji in the top right corner of the webcam and another dropdown menu where the user can choose which face detector is used in the backend for the emoji. This menu was implemented so we could test changes on the detectors more easily.\n",
    "<br/>\n",
    "<div>\n",
    "    <img src=\"./assets/settings-menu.jpg\" alt=\"Settings Menu\" width=\"320\"/>\n",
    "</div>\n",
    "\n",
    "### Image fullscreen mode\n",
    "When the user is locked on the gallery (last section) the user can now tilt their head downwards to enlarge an image of the gallery. This feature allows the user to view their photos more conveniently. By closing the right or left eye the user can go to the next or previous photo while being in the fullscreen mode."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "920a82e6fbc1529d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final state of project\n",
    "On this date we finished the prototype of our project.\n",
    "\n",
    "### Final command list\n",
    "#### Navigate through the page:\n",
    "<ul>\n",
    "    <li>Unlock area/Lock area: tilt your head up (or close both eyes)</li>\n",
    "    <li>Scroll to next area: tilt your head to the right</li>\n",
    "    <li>Scroll to previous area: tilt your head to the left</li>\n",
    "</ul>\n",
    " \n",
    "#### Navigate the carousel:\n",
    "<ul>\n",
    "    <li>Go to next photo: close your right eye</li>\n",
    "    <li>Go to previous photo: close your left eye</li>\n",
    "</ul>\n",
    "\n",
    "#### Navigate the filters:\n",
    "<ul>\n",
    "    <li>Select filter or undo: tilt your head to the right/left to go to next/previous filter</li>\n",
    "    <li>Apply filter or undo: tilt your head down</li>\n",
    "</ul>\n",
    "\n",
    "#### Navigate the gallery:\n",
    "<ul>\n",
    "    <li>Go to next photo: Close your right eye</li>\n",
    "    <li>Go to previous photo: Close your left eye</li>\n",
    "    <li>View photo bigger: tilt your head down</li>\n",
    "</ul>\n",
    "\n",
    "<p>The Help section on the left side also displays the command list and is dynamic which means it will change depending on which section the user is on. The only area that can not be controlled by eye or head movements is the upload area due to browser safety measures. Overall we achieved our goal of creating an application which can be operated almost entirely by using the face and head.</p>\n",
    "<br/>\n",
    "<div>\n",
    "    <img src=\"./assets/final-prototype-01.png\" alt=\"Final status of the prototype carousel\" width=\"640\"/>\n",
    "</div>\n",
    "<br/>\n",
    "<div>\n",
    "    <img src=\"./assets/final-prototype-02.png\" alt=\"Final status of the prototype upload area and gallery\" width=\"640\"/>\n",
    "</div>\n",
    "<br/>\n",
    "<div>\n",
    "    <img src=\"./assets/final-prototype-03.png\" alt=\"Final status of the prototype gallery\" width=\"640\"/>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a4624be8149b57b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
