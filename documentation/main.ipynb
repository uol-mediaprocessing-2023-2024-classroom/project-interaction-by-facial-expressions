{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Documentation of the project Interaction by Facial Expressions\n",
    "**Group members:** Lea Krawczyk, Thi Mai Linh Nguyen, Sebastian Vittinghoff"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Our Goal...\n",
    "\n",
    "...is to create a project where the user can **operate a photo gallery and image manipulation website (almost) entirely by using the face and head.**\n",
    "\n",
    "Additionally to our main goal we also defined other goals our website should fullfill which are:\n",
    "\n",
    "## Ease of use\n",
    "\n",
    "Our website should not need additional equipment to control and navigate through the website. The only equipment which we want to utilize in our project is the webcam which will translate head movements and facial expressions to commands triggering actions in our application. It is important for us that the navigational commands should be as intuitive as possible. Therefore we tried to map directional movements of the eyes to directional movements on our website e.g. looking to the left corresponds with moving to the left photo on the website. The last aspect in ease of use is that the app should not be physically painful to use e.g. controlling the application via gaze tracking for a longer period of time can be straining on the eyes.\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "In order to trigger commands reliably our application needs to interpret the user's actions with a high accuracy. So we needed to test the models we are going to use to ensure that they were accurate in real-time applications. We also aimed to strike a balance between detecting user actions and identifying when the application processes input e.g. closing the right eye should not trigger a command each millisecond the eye is closed but neither should it take more than a minute for a command to be triggered.\n",
    "\n",
    "## Real-time compatibility\n",
    "\n",
    "There are face recognition models that work great on images or videos but not necessarily in real-time. There are also models that take a lot of resources and work very accurately but can be overwhelming for a real-time application or our hardware. So for each command and model we implement we have to ensure that the performance is still smooth."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Progress Summary\n",
    "\n",
    "This section gives a brief summary of the progress of this project up to a specific date. Each progress date will be explained in further detail if the link is clicked.\n",
    "\n",
    "## 08.11.2023 ([Main Concept](./1-main-concept.ipynb))\n",
    "\n",
    "The main focus was to organise our group and come to an agreement regarding the concept and requirements of our project.\n",
    "\n",
    "In order to keep track of our tasks and work distribution we decided to use an online project organization tool called [Trello](https://trello.com).\n",
    "We collectively decided to work on our own implementations of the frontend and backend. The structure of our repository includes a frontend, backend and documentation folder. We also chose to work with PyCharm as our main IDE.\n",
    "\n",
    "After the basic setup was completed we discussed our goals for this project and which requirements to implement.\n",
    "\n",
    "## 22.11.2023 ([Base Project](./2-base-project.ipynb))\n",
    "\n",
    "We created a basis for our project. In the backend we added the filters \"blur\", \"black and white\" and \"invert\" and in the frontend we added the help section, a webcam, an action log and an area to modify the photos. Generally we experimented with filters OpenCV provides and adjusted the parameters to our liking. As of now we can only apply the filters on example images in the modifying area. It's not possible to upload a photo yet. Furthermore there is no gallery implemented yet.\n",
    "\n",
    "## 06.12.2023 ([Eye Detection with Dlib](./3-eye-detection-with-dlib.ipynb))\n",
    "\n",
    "We experimented with [dlib-Models](https://github.com/davisking/dlib-models) to choose an optimal model for eye and face detection. During our tests we added threshold values which are used to determine whether the left or right eye is closed. Additionally we tried to count the amount of \"blinking with the eyes\". This idea came up since we think that the amount of blinkings could be used for future commands.\n",
    "\n",
    "However, one main problem of this model was the inconsistency of the eye detection. The threshold values were changing too much which in turn made it harder to detect whether an eye is closed or not. In the future we want to research other models and adjust our implementation to get more consistent results of eye detection.\n",
    "\n",
    "## 20.12.2023 ([Switching to MediaPipe](./4-switching-to-mediapipe.ipynb))\n",
    "\n",
    "Since the eye detection results of Dlib were not consistent enough for our liking, we looked into [MediaPipe](https://developers.google.com/mediapipe). This gave us similar results to Dlib at first until we discovered that settings can be changed within MediaPipe as well. Now that we experimented with this new model, we did get a more consistent detection of closed eyes. Additionally we looked into head pose detection and gaze tracking, also using MediaPipe, which we can use for additional commands.\n",
    "\n",
    "## 16.01.2024 ([Emotion Detection with DeepFace](./5-emotion-detection-with-deepface.ipynb))\n",
    "\n",
    "Our project focuses on operating a photo gallery and image manipulation website (almost) entirely by using the face and head. Therefore we looked into other interactions we could use as possible commands. We settled on emotion detection and decided on using [DeepFace](https://github.com/serengil/deepface) in our project to detect which emotion is displayed by the user. According to the user's emotion an emoji can be controlled. We opted against utilizing emotion detection to navigate through our website due to its inaccuracy, which would make controlling elements of our application frustrating.\n",
    "\n",
    "The UI design was also updated and layout improvements were added. The Help section is now dynamic and has been updated with the available commands and its descriptions.\n",
    "\n",
    "## 30.01.2024 ([Speedtest & Finishing](./6-speedtest-and-finishing.ipynb))\n",
    "\n",
    "During the implementation of the emotion detection functionality, we noticed a drop in processing speed on our website. So we conducted a speed test in which we tested all available face detector backends of DeepFace (OpenCV, SSD, Dlib, MTCNN, Faster MTCNN, RetinaFace, MediaPipe, YOLOv8 Face, YuNet) with our application. The result was that RetinaFace and MTCNN are slowing down the application the most while OpenCV and a faster variant of MTCNN (facenet-pytorch) are the most consistent in detecting the correct emotion displayed by a user and need less than 1 second to detect a face and an emotion. Using RetinaFace or MTCNN in the background can take up to more than 20 seconds to display a change in emotion on the frontend which is not ideal for an application which relies on real-time reactions to navigate through it.\n"
   ],
   "metadata": {
    "id": "O8nNgMmstbWp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Known Issues & Limitations\n",
    "\n",
    "- The Webcam starts lagging after a while.\n",
    "- If the camera is set too high or too low, it is recognized that the head is either lowered or raised all the time. This can lead to interactions being carried out that are not intended.\n",
    "- The emotion detection often recognizes the wrong emotion."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Outlook & Optimization thoughts\n",
    "\n",
    "Our project could be improved in multiple ways.\n",
    "\n",
    "Concerning the emotion detection: Since DeepFace uses an AI model which was trained on the [FER2013](https://www.kaggle.com/datasets/msambare/fer2013) dataset we could improve the accuracy of the emotion recognition model by training our own model with our own datasets consisting of bigger images.\n",
    "\n",
    "Concerning performance speed: We need to optimize the communication between frontend and backend so the application can work accurately in real-time. One possibility would be to analyze and process the images directly in the frontend to save the detour via a node such as the server. This would lead to a considerable performance boost. The resources that would have to be transferred for this would be OpenCV and MediaPipe. Another option would be to provide a remote server with increased computing power. This would be able to process the requests alot more efficiently and take over the very computationally intensive analyses and process them faster.\n",
    "\n",
    "A useful extension for the project would be, for example, adding a setup step before starting the actual application. This would allow the user to assign the triggers to the interactions individually.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task distribution\n",
    "\n",
    "The following overview is about the tasks and topics each of us has looked into and worked on during this project. Even though this looks like a clear divide in tasks we also helped each other and worked synergistically on them. So this can be understood as an overview of the task someone mainly worked on.\n",
    "\n",
    "## Lea\n",
    "\n",
    "- Concept\n",
    "- Filters\n",
    "- Documentation\n",
    "- UI Design\n",
    "- Face & Eye Blink Detection (Dlib & MediaPipe)\n",
    "- Gaze Detection\n",
    "\n",
    "## Mai Linh\n",
    "- Concept\n",
    "- Filters\n",
    "- Documentation\n",
    "- Face & Eye Blink Detection (Dlib)\n",
    "- Emotion Detection (DeepFace) & additional Speedtest\n",
    "- Emoji Design\n",
    "\n",
    "## Sebastian\n",
    "- Concept\n",
    "- Filters\n",
    "- Documentation\n",
    "- Base Project & Architecture\n",
    "- Dependency Management\n",
    "- Head Pose Detection (MediaPipe)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
